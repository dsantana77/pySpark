{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GyBbZL20eyl1",
    "outputId": "e94c702a-93e8-41b9-f18d-ba3674dc6d19"
   },
   "outputs": [],
   "source": [
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Gy8wGrnDfIpB"
   },
   "outputs": [],
   "source": [
    "#iniciar o pyspark com Python\n",
    "#importar SparkSession - Referencia comunicação com a máquina(cluster)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import round\n",
    "from pyspark.sql.types import StructType,  StructField, IntegerType, DoubleType, StringType, TimestampType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html?highlight=sparksession#pyspark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "a2sds0pkfVYr"
   },
   "outputs": [],
   "source": [
    "# using SparkSession to create a Spark session\n",
    "spark = (\n",
    "\n",
    "    SparkSession.builder\n",
    "\n",
    "        #.master(\"local\")\n",
    "\n",
    "        .appName(\"First DF\")\n",
    "\n",
    "        #.config(\"spark.some.config.option\", \"some-value\")\n",
    "\n",
    "        .getOrCreate()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformar em DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.csv('rdd_dataset.txt')\n",
    "# df1 = spark.read.csv('base_de_dados.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('rdd_dataset.txt',\n",
    "                    \n",
    "                    sep=';',\n",
    "                   )\n",
    "df1 = spark.read.csv('base_de_dados.csv',\n",
    "                      header = True,\n",
    "                     sep=';',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|         _c0|\n",
      "+------------+\n",
      "|     Cenoura|\n",
      "|    Otimista|\n",
      "|   Solitário|\n",
      "| Imperfeição|\n",
      "|  Descoberta|\n",
      "|    Fantasia|\n",
      "|         DNC|\n",
      "| Maravilhoso|\n",
      "|Criatividade|\n",
      "| Compreensão|\n",
      "|    Atraente|\n",
      "|       Festa|\n",
      "|    Intenção|\n",
      "|    Encontro|\n",
      "|     Destino|\n",
      "|     Sucesso|\n",
      "|  Conquistar|\n",
      "|Simplicidade|\n",
      "|         Paz|\n",
      "|  Existência|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+--------------+---------------+----------------+\n",
      "| id|   valor| parte_debitada_nome|parte_debitada_conta|parte_debitada_banco|parte_creditada_nome|parte_creditada_conta|parte_creditada_banco|chave_pix_tipo|chave_pix_valor|  data_transacao|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+--------------+---------------+----------------+\n",
      "|  1|    9.93|Dra. Ana Carolina...|            79470453|              Nubank|       Maysa da Cruz|             67162333|                 Itau|           cpf|     8439752610|18/02/2022 13:28|\n",
      "|  2|   15.38|        Ana Caldeira|            19689668|                Itau|        Evelyn Sales|             60005091|             Bradesco|           cpf|    27145380617|08/04/2022 01:47|\n",
      "|  3|   57.58|    Arthur Goncalves|            18856899|            Bradesco|          Maria Melo|             13496303|                  BTG|           cpf|    16452937006|14/07/2022 03:18|\n",
      "|  4|53705.13|  Ana Julia Caldeira|            22834741|                Itau|   Ana Livia Almeida|             44695116|               Nubank|           cpf|    26590384142|15/01/2022 18:06|\n",
      "|  5|25299.69|  Srta. Nicole Pinto|             3715882|              Nubank|Srta. Ana Laura d...|             21409465|               Nubank|           cpf|    73486105280|13/05/2022 11:04|\n",
      "|  6| 7165.06|   Gabriela Ferreira|             2243037|              Nubank|       Larissa Souza|             10689552|                 Itau|           cpf|    96845371237|11/09/2022 13:38|\n",
      "|  7|    6.16|    Heloisa da Rocha|            59778949|                 BTG|Dra. Vitoria Silv...|             56583792|               Nubank|           cpf|    89064175357|10/12/2021 12:37|\n",
      "|  8|  136.36|Srta. Isadora Cor...|            77102442|              Nubank|  Francisco da Costa|             96088386|               Nubank|           cpf|    85907632429|30/12/2021 23:18|\n",
      "|  9|  574.39|   Dr. Lucas da Cruz|            38501170|                 BTG|       Calebe da Luz|             19365554|             Bradesco|           cpf|    64720189520|21/06/2021 07:20|\n",
      "| 10|   42.88|     Mirella Martins|            29535709|            Bradesco|        Danilo Lopes|             60064650|                 Itau|           cpf|    87014935232|21/09/2022 17:19|\n",
      "| 11|33629.97|Sr. Vitor Gabriel...|            67010663|                 BTG|Sra. Lavinia Cald...|             48145941|               Nubank|           cpf|    63542098124|12/09/2022 00:29|\n",
      "| 12| 4374.56|      Nathan Peixoto|            22975623|              Nubank|        Diogo da Luz|             30302218|             Bradesco|           cpf|    72908154323|07/08/2022 17:01|\n",
      "| 13|  507.18|       Miguel Araujo|            75113657|              Nubank|Marcos Vinicius G...|             67418115|                 Itau|           cpf|    84763129031|07/03/2021 12:34|\n",
      "| 14|67758.87|     Juliana Correia|             4495167|                Itau|    Davi Lucas Porto|             94395923|                  BTG|           cpf|    97804215649|24/03/2021 22:58|\n",
      "| 15|  815.53|     Ana Laura Souza|            79650252|                Itau|        Isabel Costa|             28762988|                  BTG|           cpf|    51824039689|21/02/2022 11:25|\n",
      "| 16|    2.73|           Levi Lima|            73815441|                 BTG|Dra. Maria Luiza ...|             96594203|             Bradesco|           cpf|    94516738066|20/07/2021 09:17|\n",
      "| 17|    0.54|        Otavio Cunha|            85583961|            Bradesco|       Elisa Moreira|             97003354|             Bradesco|           cpf|    15248769094|16/02/2022 10:16|\n",
      "| 18|49836.72|Ana Carolina Oliv...|            80200942|                Itau|    Stella Fernandes|             31579145|                  BTG|           cpf|    47609381250|18/07/2022 22:46|\n",
      "| 19|    9.68|        Levi Martins|            12349481|                Itau|Joao Guilherme Me...|             31102492|                  BTG|       celular|    11916824404|26/02/2022 15:05|\n",
      "| 20| 9837.22|          Noah Cunha|            84622162|            Bradesco|         Juan Mendes|             97805965|             Bradesco|       celular|    11944547225|22/06/2021 05:39|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+--------------+---------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- valor: string (nullable = true)\n",
      " |-- parte_debitada_nome: string (nullable = true)\n",
      " |-- parte_debitada_conta: string (nullable = true)\n",
      " |-- parte_debitada_banco: string (nullable = true)\n",
      " |-- parte_creditada_nome: string (nullable = true)\n",
      " |-- parte_creditada_conta: string (nullable = true)\n",
      " |-- parte_creditada_banco: string (nullable = true)\n",
      " |-- chave_pix_tipo: string (nullable = true)\n",
      " |-- chave_pix_valor: string (nullable = true)\n",
      " |-- data_transacao: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_csv = 'base_de_dados.csv'\n",
    "\n",
    "\n",
    "df1 = spark.read.csv(\n",
    "                    path=caminho_csv,\n",
    "                    header = True,\n",
    "                    sep=';',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'valor',\n",
       " 'parte_debitada_nome',\n",
       " 'parte_debitada_conta',\n",
       " 'parte_debitada_banco',\n",
       " 'parte_creditada_nome',\n",
       " 'parte_creditada_conta',\n",
       " 'parte_creditada_banco',\n",
       " 'chave_pix_tipo',\n",
       " 'chave_pix_valor',\n",
       " 'data_transacao']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See Columns\n",
    "df1.schema.fieldNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data type adjustments\n",
    "need: from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_pix = StructType([\n",
    "    StructField('id', IntegerType()),\n",
    "    StructField('valor', DoubleType()),\n",
    "])\n",
    "df2 = spark.read.csv(\n",
    "                    path=caminho_csv,\n",
    "                    header = True,\n",
    "                    sep=';',\n",
    "                    schema=schema_pix\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|   valor|\n",
      "+---+--------+\n",
      "|  1|    9.93|\n",
      "|  2|   15.38|\n",
      "|  3|   57.58|\n",
      "|  4|53705.13|\n",
      "|  5|25299.69|\n",
      "|  6| 7165.06|\n",
      "|  7|    6.16|\n",
      "|  8|  136.36|\n",
      "|  9|  574.39|\n",
      "| 10|   42.88|\n",
      "| 11|33629.97|\n",
      "| 12| 4374.56|\n",
      "| 13|  507.18|\n",
      "| 14|67758.87|\n",
      "| 15|  815.53|\n",
      "| 16|    2.73|\n",
      "| 17|    0.54|\n",
      "| 18|49836.72|\n",
      "| 19|    9.68|\n",
      "| 20| 9837.22|\n",
      "+---+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- valor: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'valor',\n",
       " 'parte_debitada_nome',\n",
       " 'parte_debitada_conta',\n",
       " 'parte_debitada_banco',\n",
       " 'parte_creditada_nome',\n",
       " 'parte_creditada_conta',\n",
       " 'parte_creditada_banco',\n",
       " 'chave_pix_tipo',\n",
       " 'chave_pix_valor',\n",
       " 'data_transacao']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.schema.fieldNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_pix = StructType([\n",
    "    StructField('id', IntegerType()),\n",
    "    StructField('valor', DoubleType()),\n",
    "    StructField('parte_debitada_nome', StringType()),\n",
    "    StructField('parte_debitada_conta', StringType()),\n",
    "    StructField('parte_debitada_banco', StringType()),\n",
    "    StructField('parte_creditada_nome', StringType()),\n",
    "    StructField('parte_creditada_conta', StringType()),\n",
    "    StructField('parte_creditada_banco', StringType()),\n",
    "    StructField('chave_pix_tipo', StringType()),\n",
    "    StructField('chave_pix_valor', StringType()),\n",
    "    StructField('data_transacao', TimestampType())    \n",
    "])\n",
    "df3 = spark.read.csv(\n",
    "                    path=caminho_csv,\n",
    "                    header = True,\n",
    "                    sep=';',\n",
    "                    schema=schema_pix,\n",
    "                    timestampFormat=\"dd/MM/yyyy HH:mm\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- valor: double (nullable = true)\n",
      " |-- parte_debitada_nome: string (nullable = true)\n",
      " |-- parte_debitada_conta: string (nullable = true)\n",
      " |-- parte_debitada_banco: string (nullable = true)\n",
      " |-- parte_creditada_nome: string (nullable = true)\n",
      " |-- parte_creditada_conta: string (nullable = true)\n",
      " |-- parte_creditada_banco: string (nullable = true)\n",
      " |-- chave_pix_tipo: string (nullable = true)\n",
      " |-- chave_pix_valor: string (nullable = true)\n",
      " |-- data_transacao: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+--------------+---------------+-------------------+\n",
      "| id|   valor| parte_debitada_nome|parte_debitada_conta|parte_debitada_banco|parte_creditada_nome|parte_creditada_conta|parte_creditada_banco|chave_pix_tipo|chave_pix_valor|     data_transacao|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+--------------+---------------+-------------------+\n",
      "|  1|    9.93|Dra. Ana Carolina...|            79470453|              Nubank|       Maysa da Cruz|             67162333|                 Itau|           cpf|     8439752610|2022-02-18 13:28:00|\n",
      "|  2|   15.38|        Ana Caldeira|            19689668|                Itau|        Evelyn Sales|             60005091|             Bradesco|           cpf|    27145380617|2022-04-08 01:47:00|\n",
      "|  3|   57.58|    Arthur Goncalves|            18856899|            Bradesco|          Maria Melo|             13496303|                  BTG|           cpf|    16452937006|2022-07-14 03:18:00|\n",
      "|  4|53705.13|  Ana Julia Caldeira|            22834741|                Itau|   Ana Livia Almeida|             44695116|               Nubank|           cpf|    26590384142|2022-01-15 18:06:00|\n",
      "|  5|25299.69|  Srta. Nicole Pinto|             3715882|              Nubank|Srta. Ana Laura d...|             21409465|               Nubank|           cpf|    73486105280|2022-05-13 11:04:00|\n",
      "|  6| 7165.06|   Gabriela Ferreira|             2243037|              Nubank|       Larissa Souza|             10689552|                 Itau|           cpf|    96845371237|2022-09-11 13:38:00|\n",
      "|  7|    6.16|    Heloisa da Rocha|            59778949|                 BTG|Dra. Vitoria Silv...|             56583792|               Nubank|           cpf|    89064175357|2021-12-10 12:37:00|\n",
      "|  8|  136.36|Srta. Isadora Cor...|            77102442|              Nubank|  Francisco da Costa|             96088386|               Nubank|           cpf|    85907632429|2021-12-30 23:18:00|\n",
      "|  9|  574.39|   Dr. Lucas da Cruz|            38501170|                 BTG|       Calebe da Luz|             19365554|             Bradesco|           cpf|    64720189520|2021-06-21 07:20:00|\n",
      "| 10|   42.88|     Mirella Martins|            29535709|            Bradesco|        Danilo Lopes|             60064650|                 Itau|           cpf|    87014935232|2022-09-21 17:19:00|\n",
      "| 11|33629.97|Sr. Vitor Gabriel...|            67010663|                 BTG|Sra. Lavinia Cald...|             48145941|               Nubank|           cpf|    63542098124|2022-09-12 00:29:00|\n",
      "| 12| 4374.56|      Nathan Peixoto|            22975623|              Nubank|        Diogo da Luz|             30302218|             Bradesco|           cpf|    72908154323|2022-08-07 17:01:00|\n",
      "| 13|  507.18|       Miguel Araujo|            75113657|              Nubank|Marcos Vinicius G...|             67418115|                 Itau|           cpf|    84763129031|2021-03-07 12:34:00|\n",
      "| 14|67758.87|     Juliana Correia|             4495167|                Itau|    Davi Lucas Porto|             94395923|                  BTG|           cpf|    97804215649|2021-03-24 22:58:00|\n",
      "| 15|  815.53|     Ana Laura Souza|            79650252|                Itau|        Isabel Costa|             28762988|                  BTG|           cpf|    51824039689|2022-02-21 11:25:00|\n",
      "| 16|    2.73|           Levi Lima|            73815441|                 BTG|Dra. Maria Luiza ...|             96594203|             Bradesco|           cpf|    94516738066|2021-07-20 09:17:00|\n",
      "| 17|    0.54|        Otavio Cunha|            85583961|            Bradesco|       Elisa Moreira|             97003354|             Bradesco|           cpf|    15248769094|2022-02-16 10:16:00|\n",
      "| 18|49836.72|Ana Carolina Oliv...|            80200942|                Itau|    Stella Fernandes|             31579145|                  BTG|           cpf|    47609381250|2022-07-18 22:46:00|\n",
      "| 19|    9.68|        Levi Martins|            12349481|                Itau|Joao Guilherme Me...|             31102492|                  BTG|       celular|    11916824404|2022-02-26 15:05:00|\n",
      "| 20| 9837.22|          Noah Cunha|            84622162|            Bradesco|         Juan Mendes|             97805965|             Bradesco|       celular|    11944547225|2021-06-22 05:39:00|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+--------------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inferSchema=True - automatic indentification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- valor: double (nullable = true)\n",
      " |-- parte_debitada_nome: string (nullable = true)\n",
      " |-- parte_debitada_conta: integer (nullable = true)\n",
      " |-- parte_debitada_banco: string (nullable = true)\n",
      " |-- parte_creditada_nome: string (nullable = true)\n",
      " |-- parte_creditada_conta: integer (nullable = true)\n",
      " |-- parte_creditada_banco: string (nullable = true)\n",
      " |-- chave_pix_tipo: string (nullable = true)\n",
      " |-- chave_pix_valor: string (nullable = true)\n",
      " |-- data_transacao: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = spark.read.csv(\n",
    "    path=caminho_csv,\n",
    "    header=True,\n",
    "    sep=';',\n",
    "    inferSchema=True\n",
    ")\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "df_cast = df1.withColumn(\n",
    "    'id', col('id').cast('int')\n",
    ").withColumn(\n",
    "    'valor', col('valor').cast('double')\n",
    ").withColumn(\n",
    "    'data_transacao', to_timestamp('data_transacao', 'dd/MM/yyyy HH:mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- valor: double (nullable = true)\n",
      " |-- parte_debitada_nome: string (nullable = true)\n",
      " |-- parte_debitada_conta: string (nullable = true)\n",
      " |-- parte_debitada_banco: string (nullable = true)\n",
      " |-- parte_creditada_nome: string (nullable = true)\n",
      " |-- parte_creditada_conta: string (nullable = true)\n",
      " |-- parte_creditada_banco: string (nullable = true)\n",
      " |-- chave_pix_tipo: string (nullable = true)\n",
      " |-- chave_pix_valor: string (nullable = true)\n",
      " |-- data_transacao: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cast.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation - Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|   valor|\n",
      "+---+--------+\n",
      "|  1|    9.93|\n",
      "|  2|   15.38|\n",
      "|  3|   57.58|\n",
      "|  4|53705.13|\n",
      "|  5|25299.69|\n",
      "|  6| 7165.06|\n",
      "|  7|    6.16|\n",
      "|  8|  136.36|\n",
      "|  9|  574.39|\n",
      "| 10|   42.88|\n",
      "| 11|33629.97|\n",
      "| 12| 4374.56|\n",
      "| 13|  507.18|\n",
      "| 14|67758.87|\n",
      "| 15|  815.53|\n",
      "| 16|    2.73|\n",
      "| 17|    0.54|\n",
      "| 18|49836.72|\n",
      "| 19|    9.68|\n",
      "| 20| 9837.22|\n",
      "+---+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cast.select('id', 'valor').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Columns + round\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dolar = df_cast.select('id', 'valor').withColumn('valor_dolar', round(col('valor')*5.05, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----------+\n",
      "| id|   valor|valor_dolar|\n",
      "+---+--------+-----------+\n",
      "|  1|    9.93|      50.15|\n",
      "|  2|   15.38|      77.67|\n",
      "|  3|   57.58|     290.78|\n",
      "|  4|53705.13|  271210.91|\n",
      "|  5|25299.69|  127763.43|\n",
      "|  6| 7165.06|   36183.55|\n",
      "|  7|    6.16|      31.11|\n",
      "|  8|  136.36|     688.62|\n",
      "|  9|  574.39|    2900.67|\n",
      "| 10|   42.88|     216.54|\n",
      "| 11|33629.97|  169831.35|\n",
      "| 12| 4374.56|   22091.53|\n",
      "| 13|  507.18|    2561.26|\n",
      "| 14|67758.87|  342182.29|\n",
      "| 15|  815.53|    4118.43|\n",
      "| 16|    2.73|      13.79|\n",
      "| 17|    0.54|       2.73|\n",
      "| 18|49836.72|  251675.44|\n",
      "| 19|    9.68|      48.88|\n",
      "| 20| 9837.22|   49677.96|\n",
      "+---+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dolar.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|valor_dolar|\n",
      "+---+-----------+\n",
      "|  1|      50.15|\n",
      "|  2|      77.67|\n",
      "|  3|     290.78|\n",
      "|  4|  271210.91|\n",
      "|  5|  127763.43|\n",
      "|  6|   36183.55|\n",
      "|  7|      31.11|\n",
      "|  8|     688.62|\n",
      "|  9|    2900.67|\n",
      "| 10|     216.54|\n",
      "| 11|  169831.35|\n",
      "| 12|   22091.53|\n",
      "| 13|    2561.26|\n",
      "| 14|  342182.29|\n",
      "| 15|    4118.43|\n",
      "| 16|      13.79|\n",
      "| 17|       2.73|\n",
      "| 18|  251675.44|\n",
      "| 19|      48.88|\n",
      "| 20|   49677.96|\n",
      "+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dolar.drop('valor').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical Operators - Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+\n",
      "| id|   valor|chave_pix_tipo|\n",
      "+---+--------+--------------+\n",
      "|  1|    9.93|           cpf|\n",
      "|  2|   15.38|           cpf|\n",
      "|  3|   57.58|           cpf|\n",
      "|  4|53705.13|           cpf|\n",
      "|  5|25299.69|           cpf|\n",
      "|  6| 7165.06|           cpf|\n",
      "|  7|    6.16|           cpf|\n",
      "|  8|  136.36|           cpf|\n",
      "|  9|  574.39|           cpf|\n",
      "| 10|   42.88|           cpf|\n",
      "| 11|33629.97|           cpf|\n",
      "| 12| 4374.56|           cpf|\n",
      "| 13|  507.18|           cpf|\n",
      "| 14|67758.87|           cpf|\n",
      "| 15|  815.53|           cpf|\n",
      "| 16|    2.73|           cpf|\n",
      "| 17|    0.54|           cpf|\n",
      "| 18|49836.72|           cpf|\n",
      "| 41|   60.55|           cpf|\n",
      "| 42|    6.62|           cpf|\n",
      "+---+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cast.select('id', 'valor', 'chave_pix_tipo').filter(col('chave_pix_tipo') == 'cpf').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+\n",
      "| id|   valor|chave_pix_tipo|\n",
      "+---+--------+--------------+\n",
      "| 19|    9.68|       celular|\n",
      "| 20| 9837.22|       celular|\n",
      "| 21|    9.36|       celular|\n",
      "| 22|   22.43|       celular|\n",
      "| 23|    7.44|       celular|\n",
      "| 24|   40.36|       celular|\n",
      "| 25|   28.66|       celular|\n",
      "| 26|  154.98|       celular|\n",
      "| 27|35859.11|       celular|\n",
      "| 28|   89.94|       celular|\n",
      "| 29|  890.47|       celular|\n",
      "| 30| 3035.83|       celular|\n",
      "| 31|20875.64|       celular|\n",
      "| 32| 1508.83|       celular|\n",
      "| 33|    1.58|       celular|\n",
      "| 34|58083.62|       celular|\n",
      "| 35| 7944.02|       celular|\n",
      "| 36|48714.95|       celular|\n",
      "| 37|19799.16|       celular|\n",
      "| 38|   32.79|       celular|\n",
      "+---+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cast.select('id', 'valor', 'chave_pix_tipo').filter(col('chave_pix_tipo') != 'cpf').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+\n",
      "| id|   valor|chave_pix_tipo|\n",
      "+---+--------+--------------+\n",
      "|  4|53705.13|           cpf|\n",
      "|  5|25299.69|           cpf|\n",
      "| 11|33629.97|           cpf|\n",
      "| 14|67758.87|           cpf|\n",
      "| 18|49836.72|           cpf|\n",
      "| 47|38219.08|           cpf|\n",
      "| 52|60139.23|           cpf|\n",
      "| 54|95977.62|           cpf|\n",
      "| 55|35409.61|           cpf|\n",
      "| 62|57433.69|           cpf|\n",
      "| 66|21639.12|           cpf|\n",
      "| 71|80083.34|           cpf|\n",
      "+---+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cast.select('id', 'valor', 'chave_pix_tipo').filter(\n",
    "    (col('chave_pix_tipo') == 'cpf') &\n",
    "    (col('valor') > 10000)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|         _c0|\n",
      "+------------+\n",
      "|     Cenoura|\n",
      "|    Otimista|\n",
      "|   Solitário|\n",
      "| Imperfeição|\n",
      "|  Descoberta|\n",
      "|    Fantasia|\n",
      "|         DNC|\n",
      "| Maravilhoso|\n",
      "|Criatividade|\n",
      "| Compreensão|\n",
      "|    Atraente|\n",
      "|       Festa|\n",
      "|    Intenção|\n",
      "|    Encontro|\n",
      "|     Destino|\n",
      "|     Sucesso|\n",
      "|  Conquistar|\n",
      "|Simplicidade|\n",
      "|         Paz|\n",
      "|  Existência|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------------+--------------------+--------------------+-------------------------+---------------------+---------------------+--------------+---------------+-------------------+\n",
      "|id |valor   |parte_debitada_nome       |parte_debitada_conta|parte_debitada_banco|parte_creditada_nome     |parte_creditada_conta|parte_creditada_banco|chave_pix_tipo|chave_pix_valor|data_transacao     |\n",
      "+---+--------+--------------------------+--------------------+--------------------+-------------------------+---------------------+---------------------+--------------+---------------+-------------------+\n",
      "|1  |9.93    |Dra. Ana Carolina Oliveira|79470453            |Nubank              |Maysa da Cruz            |67162333             |Itau                 |cpf           |8439752610     |2022-02-18 13:28:00|\n",
      "|2  |15.38   |Ana Caldeira              |19689668            |Itau                |Evelyn Sales             |60005091             |Bradesco             |cpf           |27145380617    |2022-04-08 01:47:00|\n",
      "|3  |57.58   |Arthur Goncalves          |18856899            |Bradesco            |Maria Melo               |13496303             |BTG                  |cpf           |16452937006    |2022-07-14 03:18:00|\n",
      "|4  |53705.13|Ana Julia Caldeira        |22834741            |Itau                |Ana Livia Almeida        |44695116             |Nubank               |cpf           |26590384142    |2022-01-15 18:06:00|\n",
      "|5  |25299.69|Srta. Nicole Pinto        |3715882             |Nubank              |Srta. Ana Laura da Rocha |21409465             |Nubank               |cpf           |73486105280    |2022-05-13 11:04:00|\n",
      "|6  |7165.06 |Gabriela Ferreira         |2243037             |Nubank              |Larissa Souza            |10689552             |Itau                 |cpf           |96845371237    |2022-09-11 13:38:00|\n",
      "|7  |6.16    |Heloisa da Rocha          |59778949            |BTG                 |Dra. Vitoria Silveira    |56583792             |Nubank               |cpf           |89064175357    |2021-12-10 12:37:00|\n",
      "|8  |136.36  |Srta. Isadora Correia     |77102442            |Nubank              |Francisco da Costa       |96088386             |Nubank               |cpf           |85907632429    |2021-12-30 23:18:00|\n",
      "|9  |574.39  |Dr. Lucas da Cruz         |38501170            |BTG                 |Calebe da Luz            |19365554             |Bradesco             |cpf           |64720189520    |2021-06-21 07:20:00|\n",
      "|10 |42.88   |Mirella Martins           |29535709            |Bradesco            |Danilo Lopes             |60064650             |Itau                 |cpf           |87014935232    |2022-09-21 17:19:00|\n",
      "|11 |33629.97|Sr. Vitor Gabriel da Costa|67010663            |BTG                 |Sra. Lavinia Caldeira    |48145941             |Nubank               |cpf           |63542098124    |2022-09-12 00:29:00|\n",
      "|12 |4374.56 |Nathan Peixoto            |22975623            |Nubank              |Diogo da Luz             |30302218             |Bradesco             |cpf           |72908154323    |2022-08-07 17:01:00|\n",
      "|13 |507.18  |Miguel Araujo             |75113657            |Nubank              |Marcos Vinicius Goncalves|67418115             |Itau                 |cpf           |84763129031    |2021-03-07 12:34:00|\n",
      "|14 |67758.87|Juliana Correia           |4495167             |Itau                |Davi Lucas Porto         |94395923             |BTG                  |cpf           |97804215649    |2021-03-24 22:58:00|\n",
      "|15 |815.53  |Ana Laura Souza           |79650252            |Itau                |Isabel Costa             |28762988             |BTG                  |cpf           |51824039689    |2022-02-21 11:25:00|\n",
      "|16 |2.73    |Levi Lima                 |73815441            |BTG                 |Dra. Maria Luiza Peixoto |96594203             |Bradesco             |cpf           |94516738066    |2021-07-20 09:17:00|\n",
      "|17 |0.54    |Otavio Cunha              |85583961            |Bradesco            |Elisa Moreira            |97003354             |Bradesco             |cpf           |15248769094    |2022-02-16 10:16:00|\n",
      "|18 |49836.72|Ana Carolina Oliveira     |80200942            |Itau                |Stella Fernandes         |31579145             |BTG                  |cpf           |47609381250    |2022-07-18 22:46:00|\n",
      "|19 |9.68    |Levi Martins              |12349481            |Itau                |Joao Guilherme Mendes    |31102492             |BTG                  |celular       |11916824404    |2022-02-26 15:05:00|\n",
      "|20 |9837.22 |Noah Cunha                |84622162            |Bradesco            |Juan Mendes              |97805965             |Bradesco             |celular       |11944547225    |2021-06-22 05:39:00|\n",
      "+---+--------+--------------------------+--------------------+--------------------+-------------------------+---------------------+---------------------+--------------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cast.na.fill(0, subset=['valor']).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+--------------+---------------+-------------------+\n",
      "| id|   valor| parte_debitada_nome|parte_debitada_conta|parte_debitada_banco|parte_creditada_nome|parte_creditada_conta|parte_creditada_banco|chave_pix_tipo|chave_pix_valor|     data_transacao|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+--------------+---------------+-------------------+\n",
      "|  1|    9.93|Dra. Ana Carolina...|            79470453|              Nubank|       Maysa da Cruz|             67162333|                 Itau|           cpf|     8439752610|2022-02-18 13:28:00|\n",
      "|  2|   15.38|        Ana Caldeira|            19689668|                Itau|        Evelyn Sales|             60005091|             Bradesco|           cpf|    27145380617|2022-04-08 01:47:00|\n",
      "|  3|   57.58|    Arthur Goncalves|            18856899|            Bradesco|          Maria Melo|             13496303|                  BTG|           cpf|    16452937006|2022-07-14 03:18:00|\n",
      "|  4|53705.13|  Ana Julia Caldeira|            22834741|                Itau|   Ana Livia Almeida|             44695116|               Nubank|           cpf|    26590384142|2022-01-15 18:06:00|\n",
      "|  5|25299.69|  Srta. Nicole Pinto|             3715882|              Nubank|Srta. Ana Laura d...|             21409465|               Nubank|           cpf|    73486105280|2022-05-13 11:04:00|\n",
      "|  6| 7165.06|   Gabriela Ferreira|             2243037|              Nubank|       Larissa Souza|             10689552|                 Itau|           cpf|    96845371237|2022-09-11 13:38:00|\n",
      "| 10|   42.88|     Mirella Martins|            29535709|            Bradesco|        Danilo Lopes|             60064650|                 Itau|           cpf|    87014935232|2022-09-21 17:19:00|\n",
      "| 11|33629.97|Sr. Vitor Gabriel...|            67010663|                 BTG|Sra. Lavinia Cald...|             48145941|               Nubank|           cpf|    63542098124|2022-09-12 00:29:00|\n",
      "| 12| 4374.56|      Nathan Peixoto|            22975623|              Nubank|        Diogo da Luz|             30302218|             Bradesco|           cpf|    72908154323|2022-08-07 17:01:00|\n",
      "| 15|  815.53|     Ana Laura Souza|            79650252|                Itau|        Isabel Costa|             28762988|                  BTG|           cpf|    51824039689|2022-02-21 11:25:00|\n",
      "| 17|    0.54|        Otavio Cunha|            85583961|            Bradesco|       Elisa Moreira|             97003354|             Bradesco|           cpf|    15248769094|2022-02-16 10:16:00|\n",
      "| 18|49836.72|Ana Carolina Oliv...|            80200942|                Itau|    Stella Fernandes|             31579145|                  BTG|           cpf|    47609381250|2022-07-18 22:46:00|\n",
      "| 19|    9.68|        Levi Martins|            12349481|                Itau|Joao Guilherme Me...|             31102492|                  BTG|       celular|    11916824404|2022-02-26 15:05:00|\n",
      "| 22|   22.43|       Fernanda Melo|             3317895|              Nubank| Luiz Otavio Ribeiro|             97680466|                  BTG|       celular|    84972507365|2022-08-04 00:48:00|\n",
      "| 25|   28.66|     Leandro Freitas|            91133554|                Itau|   Valentina Pereira|              6414456|                  BTG|       celular|    71979984146|2022-08-24 15:39:00|\n",
      "| 26|  154.98|         Luiza Pires|            69839296|            Bradesco|         Pietra Melo|             42851788|                 Itau|       celular|    71908616462|2022-01-28 16:51:00|\n",
      "| 29|  890.47|Sr. Luiz Felipe S...|            20051189|            Bradesco|Enzo Gabriel Cardoso|             17302910|               Nubank|       celular|    31920761665|2022-11-19 19:25:00|\n",
      "| 31|20875.64|Dr. Joao Gabriel ...|            51847615|                 BTG|        Ana Caldeira|             69556895|             Bradesco|       celular|    84993893778|2022-04-29 10:17:00|\n",
      "| 33|    1.58|Dra. Ana Vitoria ...|             7482168|                 BTG|       Alexia Farias|             53857589|                  BTG|       celular|    81992580873|2022-07-03 23:37:00|\n",
      "| 35| 7944.02|     Clara das Neves|            74962702|                 BTG|       Stella Santos|             70949900|               Nubank|       celular|    41953574945|2022-02-02 14:41:00|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+--------------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2022 = df_cast.filter(year(col('data_transacao')) == '2022')\n",
    "df_2022.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBY + Count + Sum + AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|year|count|\n",
      "+----+-----+\n",
      "|2022|   48|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2022.withColumn('year', year(col('data_transacao'))).groupBy('year').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|chave_pix_tipo|count|\n",
      "+--------------+-----+\n",
      "|       celular|   22|\n",
      "|         email|   29|\n",
      "|           cpf|   49|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cast.select('chave_pix_tipo').groupBy('chave_pix_tipo').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|chave_pix_tipo|        sum(valor)|\n",
      "+--------------+------------------+\n",
      "|       celular|         207778.46|\n",
      "|         email|499009.38000000006|\n",
      "|           cpf| 659513.3499999997|\n",
      "+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cast.select('valor', 'chave_pix_tipo').groupBy('chave_pix_tipo').sum('valor').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|chave_pix_tipo|round(sum(valor), 2)|\n",
      "+--------------+--------------------+\n",
      "|       celular|           207778.46|\n",
      "|         email|           499009.38|\n",
      "|           cpf|           659513.35|\n",
      "+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, round, avg\n",
    "\n",
    "df_cast.select('valor', 'chave_pix_tipo') \\\n",
    "       .groupBy('chave_pix_tipo') \\\n",
    "       .agg(round(sum('valor'), 2)) \\\n",
    "       .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|chave_pix_tipo|round(avg(valor), 2)|\n",
      "+--------------+--------------------+\n",
      "|       celular|             9444.48|\n",
      "|         email|            17207.22|\n",
      "|           cpf|            13459.46|\n",
      "+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "df_cast.select('valor', 'chave_pix_tipo').groupBy('chave_pix_tipo').agg(\n",
    "    round(avg('valor'), 2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o237.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24712\\2754763763.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Salva o DataFrame como CSV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf_2022\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"overwrite\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\\\df_2022.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Salva o DataFrame como Parquet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1862\u001b[0m             \u001b[0mlineSep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1863\u001b[0m         )\n\u001b[1;32m-> 1864\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m     def orc(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o237.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "# Define o caminho da pasta onde você quer salvar os arquivos\n",
    "folder_path = r\"C:\\temp\"\n",
    "\n",
    "# Salva o DataFrame como CSV\n",
    "df_2022.write.mode(\"overwrite\").option(\"header\", \"true\").csv(folder_path + \"\\\\df_2022.csv\")\n",
    "# Salva o DataFrame como Parquet\n",
    "df_2022.write.mode(\"overwrite\").parquet(folder_path + \"\\\\df_2022.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
