{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoC24P6NmAuN"
   },
   "source": [
    "# Instalando o Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5NiqW9-xlnjc",
    "outputId": "432f2c16-4a3f-432a-c802-a3807652c786"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#==3.3.1'\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark #==3.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W5NeVueloGu2",
    "outputId": "2e08b761-2f93-40c7-bca0-3e5914febca6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip ngrok-stable-linux-amd64.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gIqFurprOZ0"
   },
   "source": [
    "# Iniciar Sessão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vcytxNzMzrnX"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# ConfigureSparkUI\n",
    "conf = SparkConf().set('spark.ui.port', '4050')\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.stop()\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder                  # Método da classe que constrói a sessão spark\n",
    "      .appName(\"Meu Primeiro App Spark\")  # Nome do App Spark\n",
    "      .getOrCreate())                     # Verifica se há uma sessão ativa, e se não há, cria uma nova sessão\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tRgaHosSo6PN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html;charset=ISO-8859-1\"/>\n",
      "<title>Error 404 Not Found</title>\n",
      "</head>\n",
      "<body><h2>HTTP ERROR 404 Not Found</h2>\n",
      "<table>\n",
      "<tr><th>URI:</th><td>/api/tunnels</td></tr>\n",
      "<tr><th>STATUS:</th><td>404</td></tr>\n",
      "<tr><th>MESSAGE:</th><td>Not Found</td></tr>\n",
      "<tr><th>SERVLET:</th><td>org.glassfish.jersey.servlet.ServletContainer-42a88a12</td></tr>\n",
      "</table>\n",
      "\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "!curl -s http://localhost:4040/api/tunnels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mPHkAtuYNO-"
   },
   "source": [
    "# SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Tww10QvPhTqx"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, TimestampType\n",
    "\n",
    "caminho_csv = \"./base_de_dados.csv\"\n",
    "\n",
    "schema_base_pix = StructType([\n",
    "    StructField('id', IntegerType()),\n",
    "    StructField('valor', DoubleType()),\n",
    "    StructField('parte_debitada_nome', StringType()),\n",
    "    StructField('parte_debitada_conta', StringType()),\n",
    "    StructField('parte_debitada_banco', StringType()),\n",
    "    StructField('parte_creditada_nome', StringType()),\n",
    "    StructField('parte_creditada_conta', StringType()),\n",
    "    StructField('parte_creditada_banco', StringType()),\n",
    "    StructField('chave_pix_tipo', StringType()),\n",
    "    StructField('chave_pix_valor', StringType()),\n",
    "    StructField('data_transacao', TimestampType())\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\n",
    "    path=caminho_csv,\n",
    "    header=True,\n",
    "    sep=\";\",\n",
    "    schema=schema_base_pix,\n",
    "    timestampFormat=\"dd/MM/yyyy HH:mm\"\n",
    ")\n",
    "spark.read.csv(\n",
    "    path=caminho_csv,\n",
    "    header=True,\n",
    "    sep=\";\",\n",
    "    schema=schema_base_pix,\n",
    "    timestampFormat=\"dd/MM/yyyy HH:mm\"\n",
    ").createOrReplaceTempView(\"base_pix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oM-eqPLQuT23",
    "outputId": "93899c68-ccf4-4fbc-fb9b-d235a56454fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+--------------+---------------+-------------------+\n",
      "| id|   valor| parte_debitada_nome|parte_debitada_conta|parte_debitada_banco|parte_creditada_nome|parte_creditada_conta|parte_creditada_banco|chave_pix_tipo|chave_pix_valor|     data_transacao|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+--------------+---------------+-------------------+\n",
      "|  1|    9.93|Dra. Ana Carolina...|            79470453|              Nubank|       Maysa da Cruz|             67162333|                 Itau|           cpf|     8439752610|2022-02-18 13:28:00|\n",
      "|  2|   15.38|        Ana Caldeira|            19689668|                Itau|        Evelyn Sales|             60005091|             Bradesco|           cpf|    27145380617|2022-04-08 01:47:00|\n",
      "|  3|   57.58|    Arthur Goncalves|            18856899|            Bradesco|          Maria Melo|             13496303|                  BTG|           cpf|    16452937006|2022-07-14 03:18:00|\n",
      "|  4|53705.13|  Ana Julia Caldeira|            22834741|                Itau|   Ana Livia Almeida|             44695116|               Nubank|           cpf|    26590384142|2022-01-15 18:06:00|\n",
      "|  5|25299.69|  Srta. Nicole Pinto|             3715882|              Nubank|Srta. Ana Laura d...|             21409465|               Nubank|           cpf|    73486105280|2022-05-13 11:04:00|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+--------------+---------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from base_pix limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYLebBzY2dWJ"
   },
   "source": [
    "Porém, como saber se a manipulação de dados com Dataframes não é mais rápida que SQL?\n",
    "\n",
    "Para isso vamos propor um group by das duas maneiras e verificar qual é o plano de execução que o spark cria. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mz1XUVMb17_y"
   },
   "outputs": [],
   "source": [
    "group_sql = spark.sql(\"select chave_pix_tipo, count(1) from base_pix group by chave_pix_tipo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GvnpVoI92Jlx"
   },
   "outputs": [],
   "source": [
    "group_dataframe = df.groupBy('chave_pix_tipo').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_i9k40-lush"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bB_IsUNh2St3",
    "outputId": "2043473d-e0c7-459c-b2c0-434b61bd6925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Group\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[chave_pix_tipo#30], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(chave_pix_tipo#30, 200), ENSURE_REQUIREMENTS, [plan_id=26]\n",
      "      +- HashAggregate(keys=[chave_pix_tipo#30], functions=[partial_count(1)])\n",
      "         +- FileScan csv [chave_pix_tipo#30] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/dsant/Downloads/Estudos DNC/spark2/SparkSQL/base_de_dad..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<chave_pix_tipo:string>\n",
      "\n",
      "\n",
      "DataFrame Group\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[chave_pix_tipo#8], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(chave_pix_tipo#8, 200), ENSURE_REQUIREMENTS, [plan_id=39]\n",
      "      +- HashAggregate(keys=[chave_pix_tipo#8], functions=[partial_count(1)])\n",
      "         +- FileScan csv [chave_pix_tipo#8] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/dsant/Downloads/Estudos DNC/spark2/SparkSQL/base_de_dad..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<chave_pix_tipo:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"SQL Group\")\n",
    "group_sql.explain()\n",
    "\n",
    "print(\"DataFrame Group\")\n",
    "group_dataframe.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7_1T-ov8mKoP",
    "outputId": "de92dfa2-4f9e-4f1c-cc38-b8d8ea79aaa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|chave_pix_tipo|        sum(valor)|\n",
      "+--------------+------------------+\n",
      "|       celular|         207778.46|\n",
      "|         email|499009.38000000006|\n",
      "|           cpf| 659513.3499999997|\n",
      "+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "  \"\"\"\n",
    "    select chave_pix_tipo, sum(valor) \n",
    "    from base_pix \n",
    "    group by 1\n",
    "  \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TkFXNzwhmrZb",
    "outputId": "4208078d-8c85-4a39-ff3d-cc3abf1df83d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|chave_pix_tipo|round(sum(valor), 2)|\n",
      "+--------------+--------------------+\n",
      "|       celular|           207778.46|\n",
      "|         email|           499009.38|\n",
      "|           cpf|           659513.35|\n",
      "+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "  \"\"\"\n",
    "    select chave_pix_tipo, round(sum(valor), 2)\n",
    "    from base_pix \n",
    "    group by 1\n",
    "  \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WmmLdhNym0pL",
    "outputId": "f6a18e5e-12c3-4786-ee4f-df0754158dee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|chave_pix_tipo|sum_valor|\n",
      "+--------------+---------+\n",
      "|       celular|207778.46|\n",
      "|         email|499009.38|\n",
      "|           cpf|659513.35|\n",
      "+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "  \"\"\"\n",
    "    select chave_pix_tipo, round(sum(valor), 2) as sum_valor\n",
    "    from base_pix \n",
    "    group by 1\n",
    "  \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nPBMgMZ-m3E_",
    "outputId": "ea17a408-3c4c-400d-f743-01426ebc1dad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|chave_pix_tipo|count|\n",
      "+--------------+-----+\n",
      "|       celular|   22|\n",
      "|         email|   29|\n",
      "|           cpf|   49|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "  \"\"\"\n",
    "    select chave_pix_tipo, count(*) as count\n",
    "    from base_pix \n",
    "    group by 1\n",
    "  \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYTJLvr0Sq-Y"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bycg9Xv2SrD9"
   },
   "source": [
    "PARA AQUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (1015198927.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\dsant\\AppData\\Local\\Temp\\ipykernel_29532\\1015198927.py\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    ).show()\u001b[0m\n\u001b[1;37m            \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "      select\n",
    "        destinatario.banco,\n",
    "        valor,\n",
    "        row_number() over (partition by destinatario.banco order by valor desc) as row_number\n",
    "      from transacoes_pix\n",
    "      limit 10\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpfsHISapCwS"
   },
   "source": [
    "CTE stands for common table expression. A CTE allows you to define a temporary named result set that available temporarily in the execution scope of a statement such as SELECT, INSERT, UPDATE, DELETE, or MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0E37n56ybULQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pu8jGKlKoJoq",
    "outputId": "b45492ea-c20a-4897-c26e-197497364f70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------------+\n",
      "|parte_creditada_banco|     data_transacao|\n",
      "+---------------------+-------------------+\n",
      "|                 Itau|2022-12-15 01:29:00|\n",
      "|                  BTG|2022-12-08 23:47:00|\n",
      "|               Nubank|2022-11-19 19:25:00|\n",
      "|             Bradesco|2022-08-07 17:01:00|\n",
      "+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "  \"\"\"\n",
    "  with base_pix_row_number as(\n",
    "    select\n",
    "      parte_creditada_banco, \n",
    "      data_transacao,\n",
    "      row_number() over (partition by parte_creditada_banco order by data_transacao desc) as row_number\n",
    "    from base_pix\n",
    "  ) select\n",
    "      parte_creditada_banco,\n",
    "      data_transacao\n",
    "    from base_pix_row_number\n",
    "    where row_number = 1\n",
    "    order by data_transacao desc\n",
    "  \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkSb46FNp3Cb"
   },
   "source": [
    "Porém, não precisa ficar limitado somente a execução de queries SQL. \n",
    "\n",
    "Podemos pegar o resultado de uma query e retorná-la para um DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "h5vx27b8pxb5"
   },
   "outputs": [],
   "source": [
    "df_window = spark.sql(\n",
    "  \"\"\"\n",
    "  with base_pix_row_number as(\n",
    "    select\n",
    "      parte_creditada_banco, \n",
    "      data_transacao,\n",
    "      row_number() over (partition by parte_creditada_banco order by data_transacao desc) as row_number\n",
    "    from base_pix\n",
    "  ) select\n",
    "      parte_creditada_banco,\n",
    "      data_transacao\n",
    "    from base_pix_row_number\n",
    "    where row_number = 1\n",
    "    order by data_transacao desc\n",
    "  \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JosM2a0Bp1Cq",
    "outputId": "068b683a-4a86-419f-8eae-41d75ff704d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------------+\n",
      "|parte_creditada_banco|     data_transacao|\n",
      "+---------------------+-------------------+\n",
      "|                 Itau|2022-12-15 01:29:00|\n",
      "|                  BTG|2022-12-08 23:47:00|\n",
      "|               Nubank|2022-11-19 19:25:00|\n",
      "|             Bradesco|2022-08-07 17:01:00|\n",
      "+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_window.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7DBXmibq97j"
   },
   "source": [
    "This opens up the true power of Spark. We can treat selectExpr as a simple way to build up\n",
    "complex expressions that create new DataFrames. In fact, we can add any valid non-aggregating\n",
    "SQL statement, and as long as the columns resolve, it will be valid!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oq-ec7scstUg"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eri84Mkgq1fK",
    "outputId": "e8452864-b344-4124-c7f7-1001cc0969e9"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `va` cannot be resolved. Did you mean one of the following? [`id`, `valor`, `chave_pix_tipo`, `chave_pix_valor`, `data_transacao`].; line 1 pos 0;\n'Project [cast(data_transacao#10 as date) AS date_data_transacao#238, 'va]\n+- Relation [id#0,valor#1,parte_debitada_nome#2,parte_debitada_conta#3,parte_debitada_banco#4,parte_creditada_nome#5,parte_creditada_conta#6,parte_creditada_banco#7,chave_pix_tipo#8,chave_pix_valor#9,data_transacao#10] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29532\\519461999.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m df.selectExpr(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"date(data_transacao) as date_data_transacao\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;34m\"va\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m ).groupBy('date_data_transacao').count().orderBy(col('count').desc()).show()\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselectExpr\u001b[1;34m(self, *expr)\u001b[0m\n\u001b[0;32m   3265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3266\u001b[0m             \u001b[0mexpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# type: ignore[assignment]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3267\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3268\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `va` cannot be resolved. Did you mean one of the following? [`id`, `valor`, `chave_pix_tipo`, `chave_pix_valor`, `data_transacao`].; line 1 pos 0;\n'Project [cast(data_transacao#10 as date) AS date_data_transacao#238, 'va]\n+- Relation [id#0,valor#1,parte_debitada_nome#2,parte_debitada_conta#3,parte_debitada_banco#4,parte_creditada_nome#5,parte_creditada_conta#6,parte_creditada_banco#7,chave_pix_tipo#8,chave_pix_valor#9,data_transacao#10] csv\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "    \"date(data_transacao) as date_data_transacao\",\n",
    "    \"va\"\n",
    ").groupBy('date_data_transacao').count().orderBy(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-ob-JQ_tPvy"
   },
   "source": [
    "Exercício\n",
    "1. Vimos que há dois dias em que houve duas transações pix. Descubra são os ids dessas transações.\n",
    "\n",
    "2. Vimos que há dois dias em que houve duas transações pix. Descubra quais chaves pix foram utilizadas para realizar as transações. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xbKknPUysbcK",
    "outputId": "ec74de15-2b40-455a-c9bb-421e0418e50b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2022, 2, 26)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_datas = spark.sql(\n",
    "  \"\"\"\n",
    "  select\n",
    "    date(data_transacao) as date_data_transacao\n",
    "  from base_pix\n",
    "  group by 1\n",
    "  having count(*) > 1\n",
    "  \"\"\"\n",
    ").collect()[0][0]\n",
    "lista_datas"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DoC24P6NmAuN"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
